{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Lab 5: Agentic Document Extraction for RAG\n",
    "\n",
    "In this lab, you will build a Retrieval-Augmented Generation (RAG) pipeline using ADE. RAG enables LLMs to answer questions about documents by retrieving relevant chunks and generating answers grounded in that context.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the three phases of RAG: Preprocess, Retrieve, and Generate\n",
    "- Set up a local ChromaDB vector database with OpenAI embeddings\n",
    "- Incorporate visual grounding into queries for traceable information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "background-cell",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Traditional keyword search fails when questions don't match exact document wording. RAG implements semantic search in three parts:\n",
    "1. Preprocess: Parse documents into chunks, embed as vectors, store in database\n",
    "2. Retrieve: Convert query to vector, find semantically similar chunks\n",
    "3. Generate: Pass retrieved chunks as context to LLM for grounded answers\n",
    "\n",
    "You will work with Apple's 10-K SEC filing (74 pages) pre-parsed using ADE (453 chunks). Each chunk includes text, bounding box coordinates, page number, and chunk type (eg text, table, figure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outline-cell",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [1. Import Libraries](#1)\n",
    "- [2. Loading ADE-Parsed Data](#2)\n",
    "- [3. Setting up the Vector Database](#3)\n",
    "- [4. Querying the Database](#4)\n",
    "- [5. Hybrid Search](#5)\n",
    "- [6. RAG with LangChain](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Import Libraries\n",
    "\n",
    "Load libraries:\n",
    "\n",
    "- **openai**: Generate vector embeddings (`text-embedding-3-small`)\n",
    "- **chromadb**: Vector database for storing and querying embeddings\n",
    "- **langchain**: Framework for building RAG chains\n",
    "\n",
    "Additionally we have a script `helper.py` containing some utilities for cropping images and superimposing bounding boxes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfa9ce-a207-4ba8-a368-688a61ffbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import display, Image, IFrame, Markdown, JSON \n",
    "\n",
    "import helper\n",
    "\n",
    "# OpenAI & ChromaDB - Embedding + Vector Store\n",
    "import openai\n",
    "import chromadb\n",
    "\n",
    "# Langchain \n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load environment variables from .env\n",
    "_ = load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Loading ADE-Parsed Data\n",
    "\n",
    "The Apple 10-K document has been pre-parsed using ADE. The output includes:\n",
    "- Markdown: Full document converted to structured markdown with anchor tags\n",
    "- JSON: Individual content chunks with metadata (bounding boxes, page numbers, chunk types)\n",
    "\n",
    "This pre-computed output lets you focus on the RAG pipeline. In production, you would call the ADE Parse API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbe79b-eb9a-4354-a7eb-0e965a794bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATH = Path(\"apple_10k.pdf\")\n",
    "helper.print_document(DOC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paths-explanation",
   "metadata": {},
   "source": [
    "Set up paths to the pre-computed ADE outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db99fa6-c04f-4205-acaa-39ff0f97c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Directories and Paths\n",
    "OUTPUT_DIR = Path(\"./ade_outputs\")\n",
    "ADE_JSON_PATH = OUTPUT_DIR / \"apple_10k_chunks.json\"  \n",
    "ADE_MD_PATH = OUTPUT_DIR / \"apple_10k.md\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-preview-explanation",
   "metadata": {},
   "source": [
    "Preview the parsed markdown. Notice the anchor IDs which linking extracted information back to source locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb88358-7f11-4185-bf71-0b8e4f0c0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display markdown preview\n",
    "print(\"\\n Parsed Output (Page 1):\")\n",
    "with open(ADE_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_content = f.read()\n",
    "    # Find first page content (up to first page break or 500 chars)\n",
    "    first_page = markdown_content[:500]\n",
    "    print(first_page + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunks-explanation",
   "metadata": {},
   "source": [
    "Load the JSON chunks. Each chunk contains:\n",
    "- `chunk_id`: Unique identifier (matches anchor IDs in markdown)\n",
    "- `chunk_type`: \"text\", \"table\", or \"figure\"\n",
    "- `text`: The actual content\n",
    "- `bbox`: Bounding box coordinates `[x0, y0, x1, y1]` (normalized 0-1)\n",
    "- `page`: Page number (0-indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64b55f-2996-41f0-a921-4562bf3da4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing JSON chunks\n",
    "with open(ADE_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_chunks = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(loaded_chunks)} saved chunks.\")\n",
    "\n",
    "# Show first chunk structure\n",
    "print(f\"\\n Sample chunk structure:\")\n",
    "print(json.dumps(loaded_chunks[0], indent=2)[:400] + \"...\")\n",
    "\n",
    "print(\"\\n Ready to query!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Setting up the Vector Database\n",
    "\n",
    "Set up ChromaDB to store vector embeddings. ChromaDB is a lightweight vector database that supports:\n",
    "- **HNSW indexing**: Fast approximate nearest neighbor search\n",
    "- **Metadata filtering**: Query by chunk type, page number, etc.\n",
    "- **Persistence**: Data survives kernel restarts\n",
    "\n",
    "You will use OpenAI's `text-embedding-3-small` to convert text into 1536-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae8027-8ce7-41c9-b355-998f05fd460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Directory and Collectioon name\n",
    "CHROMA_DB_PATH = Path(\"./chroma_db\")\n",
    "COLLECTION_NAME = \"ade_documents\"\n",
    "\n",
    "# embeding model for vector database \n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# Instantiate the Chroma Client\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "\n",
    "# Create or Load ADE Collection\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-existing-explanation",
   "metadata": {},
   "source": [
    "Check if chunks are already in the database from a previous run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04368a-5abb-487b-9ebb-4ed22883a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Checking for existing chunks in Chroma...\")\n",
    "\n",
    "# Get all existing chunk IDs from the collection\n",
    "existing_result = collection.get(\n",
    "                    ids=[chunk[\"chunk_id\"] for chunk in loaded_chunks])\n",
    "existing_ids = set(existing_result.get('ids', []))\n",
    "print(f\"Found {len(existing_ids)} existing chunks in collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-explanation",
   "metadata": {},
   "source": [
    "For each new chunk, generate an embedding and store it with metadata:\n",
    "- `chunk_type`: Filter by content type (text vs. table)\n",
    "- `page`: Filter by page number\n",
    "- `bbox_*`: Bounding box coordinates for visual grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48450a3-4276-428d-aff1-8e4151258aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inserting new chunks into Chroma...\")\n",
    "\n",
    "added_count = 0\n",
    "for i, chunk in enumerate(loaded_chunks):\n",
    "    chunk_id = chunk[\"chunk_id\"]\n",
    "    \n",
    "    # Add chunk if it does not exist\n",
    "    if chunk_id not in existing_ids:\n",
    "        text = chunk.get(\"text\", \"\")\n",
    "        \n",
    "        # Skip empty chunks\n",
    "        if not text or not text.strip():\n",
    "            continue\n",
    "\n",
    "        # Generate Embeddings for chunk text with OpenAI\n",
    "        emb = openai.embeddings.create(\n",
    "            input=text,\n",
    "            model=EMBEDDING_MODEL\n",
    "        ).data[0].embedding\n",
    "        \n",
    "        # Flatten Metadata (Simple Types Only)\n",
    "        metadata = {\n",
    "            \"chunk_type\": chunk.get(\"chunk_type\", \"unknown\"),\n",
    "            \"page\": chunk.get(\"page\", 0)\n",
    "        }\n",
    "        \n",
    "        # Add bbox coordinates to metadata\n",
    "        bbox = chunk.get(\"bbox\")\n",
    "        if bbox and len(bbox) == 4:\n",
    "            metadata[\"bbox_x0\"] = float(bbox[0])\n",
    "            metadata[\"bbox_y0\"] = float(bbox[1])\n",
    "            metadata[\"bbox_x1\"] = float(bbox[2])\n",
    "            metadata[\"bbox_y1\"] = float(bbox[3])\n",
    "        \n",
    "        # Store in Chroma\n",
    "        collection.add(\n",
    "            documents=[text],\n",
    "            ids=[chunk_id],\n",
    "            metadatas=[metadata],\n",
    "            embeddings=[emb]\n",
    "        )\n",
    "        \n",
    "        added_count += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (added_count) % 20 == 0:\n",
    "            print(f\"   Processed {added_count} new chunks...\")\n",
    "\n",
    "print(f\"\\nâœ“ Added {added_count} new chunks, skipped existing chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. Querying the Database\n",
    "\n",
    "Create a function to query the vector database:\n",
    "1. Embed the query: Convert question to vector\n",
    "2. Similarity search: Find chunks closest to query vector\n",
    "3. Filter by threshold: Return only results above minimum similarity\n",
    "4. Visual grounding: Display cropped images of source chunks\n",
    "\n",
    "Similarity score is `1 - distance`, where distance is the L2 (Euclidean) distance between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70365c7-3ea1-4084-8680-986e4ff761db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, top_k=3, threshold=0.25, show_images=True):\n",
    "    \"\"\"\n",
    "    Query the ADE Chroma index with a natural language question.\n",
    "    Dynamically extracts and displays JUST the relevant chunk from \n",
    "    the PDF.\n",
    "    \n",
    "    Args:\n",
    "        question (str): User query\n",
    "        top_k (int): Max results to return\n",
    "        threshold (float): Minimum similarity (1 - distance)\n",
    "        show_images (bool): Display chunk-level grounding visualizations\n",
    "    \"\"\"\n",
    "    # 1. Embed Query\n",
    "    q_embed = openai.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=question\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # 2. Query Chroma\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_embed],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Query: {question}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 3. Parse Results\n",
    "    retrieved_docs = results[\"documents\"][0]\n",
    "    retrieved_meta = results[\"metadatas\"][0]\n",
    "    retrieved_dists = results[\"distances\"][0]\n",
    "    retrieved_ids = results[\"ids\"][0]\n",
    "\n",
    "    found_any = False\n",
    "    for i, (text, meta, dist, cid) in enumerate(zip(\n",
    "        retrieved_docs, retrieved_meta, retrieved_dists, retrieved_ids\n",
    "    )):\n",
    "        similarity = 1 - dist\n",
    "         # Skip Weak Matches\n",
    "        if similarity >= threshold:\n",
    "            \n",
    "            found_any = True\n",
    "            page_num = meta.get('page', 0)\n",
    "            chunk_type = meta.get('chunk_type', 'unknown')\n",
    "            \n",
    "            print(f\"\\n  Result {i+1} (similarity={similarity:.3f}):\")\n",
    "            print(f\"   Chunk ID: {cid}\")\n",
    "            print(f\"   Type: {chunk_type}, Page: {page_num}\" )\n",
    "            print(f\"   Text preview: {text[:200]}...\")\n",
    "            \n",
    "            # Display chunk-level grounding image\n",
    "            if show_images:\n",
    "                # Extract bbox from metadata if available\n",
    "                bbox = None\n",
    "                if all(k in meta for k in ['bbox_x0', 'bbox_y0', 'bbox_x1', 'bbox_y1']):\n",
    "                    bbox = [\n",
    "                        meta['bbox_x0'],\n",
    "                        meta['bbox_y0'],\n",
    "                        meta['bbox_x1'],\n",
    "                        meta['bbox_y1']\n",
    "                    ]\n",
    "                \n",
    "                # Dynamically extract chunk image from PDF\n",
    "                print(f\"\\n Dynamically extracting chunk from PDF...\")\n",
    "                chunk_img = helper.extract_chunk_image(\n",
    "                    pdf_path=DOC_PATH,\n",
    "                    page_num=page_num,\n",
    "                    bbox=bbox,\n",
    "                    highlight=True,\n",
    "                    padding=10\n",
    "                )\n",
    "                \n",
    "                if chunk_img:\n",
    "                    print(f\"{chunk_type.title()} chunk (cropped):\")\n",
    "                    display(Image(data=chunk_img))\n",
    "                else:\n",
    "                    print(f\"Could not extract chunk image\")\n",
    "          \n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No results above similarity threshold.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"RAG query function defined with dynamic chunk extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-example-explanation",
   "metadata": {},
   "source": [
    "Test the query function. Visual grounding images verify the retrieved chunks contain relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e8b58-bc52-4702-8ab5-15bb2e17022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in Question in Natural Language, Top_K, and Threshold/L2 Distance\n",
    "rag_query(\"What was Appleâ€™s net sales in 2023?\", \n",
    "          top_k=5, \n",
    "          threshold=0.32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "## 5. Hybrid Search\n",
    "\n",
    "Semantic search retrieves chunks according to similarity. Hybrid search adds metadata filtering using the `where` parameter.\n",
    "\n",
    "For example, when asking about revenue figures, search only tables since financial data is typically tabular. This avoids retrieving narrative text that mentions revenue in passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606506c3-0e0c-4ed2-a34b-1d7764abe563",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embed = openai.embeddings.create(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    input=\"What was Appleâ€™s total revenue in 2023?\",\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[q_embed],\n",
    "    n_results=5,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    where = {\"chunk_type\": \"table\"},\n",
    ")\n",
    "\n",
    "results[\"documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "\n",
    "## 6. RAG \n",
    "\n",
    "Combine retrieval with generation using LangChain. The `create_retrieval_chain` builds a pipeline that:\n",
    "1. Takes a user question\n",
    "2. Retrieves relevant chunks from the vector store\n",
    "3. Formats them as context for the LLM\n",
    "4. Generates a grounded answer\n",
    "\n",
    "This abstracts the manual embedding and querying steps into a reusable chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectordb-explanation",
   "metadata": {},
   "source": [
    "First, wrap the ChromaDB collection with LangChain's `Chroma` class to get a retriever interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d5c2a-6e78-4feb-b650-70cdc917764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=OpenAIEmbeddings(model = EMBEDDING_MODEL),\n",
    "    persist_directory=str(CHROMA_DB_PATH)\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-chain-explanation",
   "metadata": {},
   "source": [
    "Create the RAG chain with a prompt template. The `{context}` placeholder will be filled with retrieved chunks:\n",
    "\n",
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their non-deterministic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dea042-c930-41c7-bc74-8b5e0285a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "system_prompt = (\n",
    "    \"Use the following pieces of retrieved context to answer the \"\n",
    "    \"user's question. \"\n",
    "    \"If you don't know the answer, say that you don't know.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature = 1)\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = create_retrieval_chain(retriever, prompt | llm)\n",
    "\n",
    "# Invoke the chain (conceptual)\n",
    "response = rag_chain.invoke({\"input\": \n",
    "                             \"What were Apple net sales in 2023\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "follow-up-explanation",
   "metadata": {},
   "source": [
    "Try a question requiring understanding trends across years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e43103-aafa-41cb-834e-7bad8fa9f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the chain (conceptual)\n",
    "response = rag_chain.invoke({\n",
    "    \"input\": \"How did total revenue trend between 2023 and 2022\" \n",
    "                                       \"for iPhone sales?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-cell",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You built a complete RAG pipeline using ADE-parsed documents:\n",
    "\n",
    "| Step | Component | Purpose |\n",
    "|------|-----------|--------|\n",
    "| **Parse** | ADE Parse API | Convert PDF to structured chunks with metadata |\n",
    "| **Embed** | OpenAI `text-embedding-3-small` | Convert text to 1536-dim vectors |\n",
    "| **Store** | ChromaDB | Persist vectors with metadata for fast retrieval |\n",
    "| **Query** | Similarity Search | Find chunks semantically similar to query |\n",
    "| **Retrieve** | Hybrid Search | Narrow results by chunk type, page, etc. |\n",
    "| **Generate** | LangChain RAG Chain | Ground LLM answers in retrieved context |\n",
    "\n",
    "Additionally, you learned to **verify** retrieved information and generated responses through visual grounding.\n",
    "\n",
    "In our next lesson, we will learn to deploy ADE in the cloud â€” scaling our RAG application through an event-driven architecture that automatically triggers ADE for document processing whenever new documents appear. This lab helped us understand the logic that will drive our production-ready pipeline. Everything you learned here about embeddings, similarity search, threshold tuning, and metadata filtering transfers directly to Lesson 6. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "L1L5",
   "language": "python",
   "name": "l1l5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
