# LangChain Ollama Integration - Start Here üöÄ

## What Was Just Set Up?

Your GVPOCR project now has **complete LangChain integration** with your Ollama server! This enables:

- ü§ñ LLM-powered document analysis
- üîç Semantic search with embeddings
- üí¨ Conversational AI with memory
- ‚ö° Streaming real-time responses
- üì¶ Batch processing capabilities

## üìã Quick Start (5 minutes)

### 1. Install Dependencies
```bash
cd backend
pip install -r requirements.txt
```

### 2. Start Ollama (if not already running)
```bash
ollama serve
# or with Docker:
docker run -d -p 11434:11434 ollama/ollama
```

### 3. Pull Models
```bash
ollama pull mistral           # LLM for text generation
ollama pull nomic-embed-text  # Embedding model
```

### 4. Start Your Flask App
```bash
python backend/run.py
```

### 5. Test It
```bash
curl http://localhost:5000/api/langchain/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "model": "mistral",
  "host": "http://ollama:11434"
}
```

## üìö Documentation Structure

Read these in order based on your needs:

### üèÉ For Quick Integration (15 mins)
‚Üí **LANGCHAIN_QUICK_REFERENCE.md**
- Install & test commands
- API endpoints
- Code snippets

### üèóÔ∏è For Full Understanding (30 mins)
‚Üí **LANGCHAIN_OLLAMA_SETUP.md**
- Complete API documentation
- Configuration options
- Architecture diagram
- Troubleshooting guide

### üí° For Code Examples (20 mins)
‚Üí **LANGCHAIN_INTEGRATION_EXAMPLES.py**
- Real-world use cases
- Flask route examples
- Integration patterns
- Error handling

### üìä For Overview (5 mins)
‚Üí **LANGCHAIN_SETUP_SUMMARY.md**
- What was added
- Features enabled
- Next steps

## üéØ Available API Endpoints

All endpoints are under `/api/langchain/`:

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/health` | Check Ollama status |
| POST | `/invoke` | Single LLM completion |
| POST | `/batch` | Multiple completions |
| POST | `/embed` | Get text embeddings |
| POST | `/chat` | Chat with memory |
| POST | `/stream` | Streaming responses |
| GET | `/config` | View configuration |

## üêç Python Usage (In Your Routes)

```python
from app.services.langchain_service import get_langchain_service

# Get the service
service = get_langchain_service()

# Single completion
response = service.invoke("What is machine learning?")

# Multiple completions
responses = service.batch_invoke(["Q1?", "Q2?"])

# Embeddings for search
embeddings = service.get_embeddings(["doc1", "doc2"])

# Chat with context
service.create_conversation_chain()
answer1 = service.chat("What is AI?")
answer2 = service.chat("Tell me more")  # Remembers context
```

## üåê REST API Usage (From Frontend)

```javascript
// Health check
const health = await fetch('/api/langchain/health').then(r => r.json());

// Get completion
const result = await fetch('/api/langchain/invoke', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({prompt: 'What is AI?'})
}).then(r => r.json());

console.log(result.response);
```

## üöÄ Common Use Cases

### 1. Document Analysis
```python
# Analyze OCR results with LLM
ocr_text = "... extracted text ..."
analysis = service.invoke(f"Summarize: {ocr_text}")
```

### 2. Semantic Search
```python
# Find similar documents
query_embedding = service.get_embedding(search_query)
doc_embeddings = service.get_embeddings(documents)
# Calculate similarity and rank results
```

### 3. Batch Processing
```python
# Process multiple documents in parallel
prompts = [f"Analyze: {doc}" for doc in documents]
results = service.batch_invoke(prompts)
```

### 4. Interactive Q&A
```python
# User asks questions about a document
service.create_conversation_chain()
answer = service.chat("What is in this document?")
followup = service.chat("Tell me more about X")  # Remembers context
```

## üîß Configuration

Edit `.env` if you want custom settings:

```bash
# Ollama server location
OLLAMA_HOST=http://ollama:11434

# LLM model (mistral, neural-chat, dolphin-mixtral, llama2)
OLLAMA_MODEL=mistral

# Embedding model (nomic-embed-text, all-minilm)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

## üìÅ Files Added/Modified

### New Files:
- ‚úÖ `backend/app/services/langchain_service.py` - Core service
- ‚úÖ `backend/app/routes/langchain_routes.py` - API endpoints
- ‚úÖ `LANGCHAIN_OLLAMA_SETUP.md` - Full documentation
- ‚úÖ `LANGCHAIN_QUICK_REFERENCE.md` - Quick ref
- ‚úÖ `LANGCHAIN_SETUP_SUMMARY.md` - Overview
- ‚úÖ `LANGCHAIN_INTEGRATION_EXAMPLES.py` - Code examples
- ‚úÖ `LANGCHAIN_START_HERE.md` - This file

### Modified Files:
- ‚úÖ `backend/requirements.txt` - Added dependencies
- ‚úÖ `backend/app/routes/__init__.py` - Registered blueprint

## ‚ö° Performance Tips

1. **Model Selection**: `mistral` is fast and good quality
2. **Batch Operations**: Process multiple items at once
3. **Caching**: Embeddings can be cached for reuse
4. **Streaming**: Use `/stream` for large responses
5. **GPU**: Ollama uses GPU if available for faster inference

## üêõ Troubleshooting

### "Cannot connect to Ollama"
```bash
# Verify Ollama is running
curl http://localhost:11434/api/tags

# Check OLLAMA_HOST in .env
echo $OLLAMA_HOST
```

### "Model not found"
```bash
# List available models
ollama list

# Pull missing model
ollama pull mistral
```

### ImportError
```bash
# Reinstall dependencies
pip install -r backend/requirements.txt --force-reinstall
```

See **LANGCHAIN_OLLAMA_SETUP.md** for more troubleshooting.

## üéì Next Steps

1. ‚úÖ **Install & test** - Follow "Quick Start" above
2. üìñ **Read documentation** - Pick relevant doc based on needs
3. üíª **Try examples** - Run code from LANGCHAIN_INTEGRATION_EXAMPLES.py
4. üîå **Integrate** - Add LangChain to your routes/workflows
5. üß™ **Test thoroughly** - Test with real data
6. üìä **Monitor** - Check Flask logs for issues

## üÜò Need Help?

1. Check relevant documentation file
2. Review code examples in LANGCHAIN_INTEGRATION_EXAMPLES.py
3. Check Flask application logs
4. Verify Ollama logs: `ollama logs`
5. Test basic connectivity: `curl http://OLLAMA_HOST:11434/api/tags`

## üìû Support Resources

| Need | File |
|------|------|
| Quick commands | LANGCHAIN_QUICK_REFERENCE.md |
| Full API docs | LANGCHAIN_OLLAMA_SETUP.md |
| Code examples | LANGCHAIN_INTEGRATION_EXAMPLES.py |
| Setup overview | LANGCHAIN_SETUP_SUMMARY.md |
| Getting started | LANGCHAIN_START_HERE.md (this file) |

---

**Status**: ‚úÖ Setup Complete and Ready to Use

**Next**: Install dependencies and test the API!

```bash
# Copy-paste quick start:
cd backend && pip install -r requirements.txt
python -c "from app.services.langchain_service import get_langchain_service; print('‚úÖ Import successful!')"
```
